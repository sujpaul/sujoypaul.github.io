---
title: "Introduction to AI Strategy and Business Value"
date: 2025-09-23
categories: [AI, Strategy, ML, DL]
tags: [strategy, AI, DL, ROI]
read_time: true          # ✅ Shows estimated reading time (e.g., "7 min read")
toc: true                # ✅ Enables Table of Contents
toc_sticky: true         # ✅ Keeps TOC visible as you scroll (nice for long posts)
---

## Introduction {#Introduction}

Digitization in organisations have brought into the forefront the
limitless potential of what can be done with the explosive amount of
data that is available within corporations. There are also other
traditional companies that have either yet to be digitised or in the
path towards digitisation. As such, the volume of data is bound to grow
further.

AI, with all the different sophisticated algorithms, is a powerful tool
that itself scoops up enormous amount of data and spew out outputs that
can bring out tremendous benefits to different organizations and hence
to society in general. Companies are grappling with how to use this
technology and marshal all the resources available to improve the
operational efficiency, increase the productivity and thereby,
exponentially, the profit margin. For all of these to happen, they must
create and then capture the value regardless of the industry they are
attached. Yet, the innovator’s dilemma of translating something tangible
that is mutually beneficial is also constrained by the industry that she
is working in. This is known as the industry effect.

Figure 1 shows the ROE and ROIC for the different categories of industry
from the period between 1999 and the current year (January 2025). The
Figure shows that across all publicly traded US companies the average
return is approximately 12 %. The data<sup>(1)</sup> considered is for
the US. This profit effect is tied to the industry that you are working
in, as shown in Figure 1.

![ROE for each of the sectors in the industry](/assets/images/intro_strategy/media/image1.png)

Figure 1. ROE for each of the sectors in the industry

The variance within each industry is large so there are companies that
made enormous profit. The average returns of some industries is
considerably higher than those of some other industries where the
average/median return in around 12%. The largest return is over 50% in
case of healthcare while there are some industries which has shown
negative returns. As Porter<sup>(2,3)</sup> in his seminal work had
mentioned that powerful forces within the industry like suppliers,
customers, product substitutes and barriers to entry in the industry
effects the way companies operate and hence makes a profit. When these
forces are fierce, for example barriers to entry are lower, then fierce
competition can have a lower profit margin. On the other hand, when
these forces are benign companies can make a windfall in their profit
margin. This is called the industry effect. You are bound by the forces
that drives the economics of the industry. So, the value that needs to
be created by AI will be determined by the domain knowledge of the
business, the capabilities, the feasibility and finally the adoption of
the technology.

However, there are limitations of using ROE as a measure. ROE measures
the return based on common equity. So, if the company also offers
preferred equity then that needs to be subtracted from the net income in
order calculate what is available for the common shareholders.

*Note: ROE =(Net Income for common shareholders)/(Common shareholders
equity)*

As such, a better measure is to use the ROIC. For technology to be
adopted there needs to be a business transformation and this itself can
be a major challenge. Figure 2 shows the ROIC across different
industries.

![ROIC by Industry](/assets/images/intro_strategy/media/image2.png)

Figure 2. ROIC by Industry

It measures how efficiently the company uses its capital investments to
generate profits. It therefore is an indicator as to how much profit is
made for every dollar invested in the business. Hence by monitoring over
time it is possible to understand the different facets of operational
performance and the strategies considered in allocating capital and
hence the overall financial health.

*Note: ROIC= (Operating Income)/(Invested capital) =
\[EBITx(1-taxrate)\]/\[net working capital+ fixed assets\]=
\[EBITx(1-taxrate)\]/\[interest bearing debt+ equity\]*

*Capital can be raised either from assets or from debt*

*EBIT: Earnings before interest and taxes*

The average ROIC is 13% across all industries while healthcare support
services have ROIC close to 45%.

Monitoring rising ROIC trends for companies shows us how effectively the
company is managed while maintaining sustainable profits. It gives us a
good indicator of how well the company is managed. Then by comparing it
to the peers you can see how well the company is doing. Each company and
the industry that it belongs to have its own capital investment
strategies and the risks associated with it. A high ROIC is therefore an
indicator of high operational efficiency( eg streamlining processes,
reducing wastes), capital investment strategies( investing in the right
capital projects that generate profits), competitive advantages like
brands, intellectual property rights like patents, efficient working
capital management like receivables and payables.

ROIC is affected by macroeconomic conditions like inflation, interest
rates. So in case of a strong economic outlook, it will likely have a
high ROIC while during downturns due to lower demands and tighter credit
controls to raise money can have a negative effect. As such, ROIC gives
a better indication of how companies make use of strategies, operational
efficiency, profitability and capital allocation. By tying all these
together, companies would be better able to provide value that will
generate better than average performance for their shareholders.

This is illustrated in the diagram of how much ROIC has an effect on the
EV/Invested Capital (Figure 3) compared to ROE (Figure 4).

![Relationship between EV/Invested Capital and ROIC](/assets/images/intro_strategy/media/image3.png)

Figure 3: Relationship between EV/Invested Capital and ROIC

The ROIC has a clear relationship with the Valuation (R<sup>2</sup> of
49%) while no relationship exists between ROE and
Valuation(R<sup>2</sup> = 5%)

![Relationship between EV/Invested Capital and ROE](/assets/images/intro_strategy/media/image4.png)

Figure 4: Relationship between EV/Invested Capital and ROE

If the organisation is siloed, data is fragmented, inter-departmental
rivalry prevents good data governance and hence the ability to track
quality of data becomes compromised. No amount of AI capability will be
able to produce the output that the business would need to provide value
to the customer. AI assets as a force multiplier, can only be created
when the data assets are trusted. In order for strategy to work, change
management also has to work. Otherwise, the detrimental effect of AI on
the reputation of the business will be impacted. Organizational culture
and change management will be discussed in detail in Chapter 3.

There are numerous benefits of AI implementation. It ranges from
increasing productivity by automation of services, unearthing new
knowledge, diagnostic imaging of cancerous tissues, creating new
materials, reducing the timeline of new drug discovery. As with any
other technology, the gains accrued can be easily offset by the pitfalls
on the other side of the ledger. Safety, security and privacy are some
of the legitimate concerns. Generative AI has the potential to generate
fake images that can be damaging. Thus, guardrails and legal frameworks
are being drafted to ensure there is protection from the harmful and
deleterious effect of AI so that businesses can harness the power of AI
to drive imagination and foster innovation and reap all its benefits.
For example, EU has already drafted a law on the usage of AI.

The value AI can generate for businesses will be determined based on the
needs, wants and desires (also known as the Voice of the Customer) of
the customers that they serve. This is illustrated in the Kano modelling
diagram(Figure 3), which shows how the satisfaction of the customers
varies with the delivery of service. How they deal with the constraints,
along with the VOC (voice of customer), will also be a major factor if
process optimization is a goal that they want to accomplish using AI.
For businesses to increase market share, they need to attract more
customers. This is normally done by introducing new product features
(desires) in their mix using AI. How AI can be used to create value that
the business will gain is where strategy comes into play.

![Kano Model](/assets/images/intro_strategy/media/image5.png)

Figure 5: Kano Model

*Note: If you are already aware of the different components of Machine
Learning (ML) and Deep Learning(DL) then you can skip the section on
Introduction to AI and move to the section on Strategy of the chapter.*

## AI Fundamentals {#Fundamentals}

Although this book is meant for those who work with the development and
implementation of strategy, it is essential that there is at least a
high-level awareness of what AI is and the different strands of AI. If
you are a Data Scientist, a Data Analyst, or in any other position where
you have to write programs in AI then you can skip this section.

### Machine Learning {#ml}

AI consists of Machine Learning (ML) and Deep Learning (DL). ML
algorithms are built based on whether data is structured, unstructured
or semi-structured. So any data that is labelled is called structured
while data that does not have any label is considered unstructured.
Semi-structured data, as you can guess it, is a combination of both.
Algorithms that deal with structured data are called Supervised ML,
unstructured data is called Unsupervised ML, semi-structured is known as
Semi-supervised ML.

Supervised Learning technique is a classification technique. For
example, the total size of all of your databases across your production
environment has been growing monthly for the past year. The budget
season is coming up soon and you have to find out what the dollar amount
you would absolutely need to make the necessary capital investments for
storage and other disaster recovery scenarios for your on-premise
servers. You are part of the IT team and other managers are also
competing with an ever decreasing share of the pie. In this case there
is only 1 dependent variable, size(TB), while there is one independent
variable, time(months). Using linear regression algorithm you can
predict what the size of your databases would be and how much storage
you will need. Knowing what the size of your data will be 6 months or 12
months from now you can then calculate, and hence, predict the costs.
This helps to alleviate waste and having the data necessary to forecast
the cost makes it transparent and instil trust within the team. Using AI
to facilitate capacity planning is one of the benefits and the spill-off
effect is using data to bring out fact-based management that employees
can trust.

Other supervised learning techniques are Logistic Regression. This is a
binary classification technique whose outcome is True/False. This
algorithm can determine whether the customer will default on loan
payment and the bank can then decide to provide the loan to the
customer. Other examples of Classification are Polynomial Regression,
Logistic Regression, Support Vector Machines (SVM), Decision Trees,
Random Forest, Factor Analysis and Principal Component Analysis(PCA).

***Note*** *Unlike linear regression where there is one 1 dependent and
1 independent variable, polynomial regression contains several dependent
variables*

The use of PCA in reducing the number of dimensions or features (also
known as feature engineering) is itself a separate topic on its own. As
human beings we can only visualize up to 3 dimensions. Higher order
dimensions help us to get more insights and accuracy of the prediction
of the algorithm. It is however necessary at times to eliminate and
focus on those dimensions that has the significant impact. Using PCA, it
was found that psychological safety (Amy Edmondson) is a key ingredient
that drives creativity and fosters innovation in organizational teams.
This importance of this topic will be discussed in more detail in the
chapter on change management.

Clustering is an example of unsupervised learning. KNN (K-Nearest
Neigbours) is a type of clustering algorithm. In this instance, number
of clusters are formed where each contains a median point. Data points
that are closer to a centroid are allocated to the particular cluster.
Using this technique, it is possible to segment the customers based on
variables like demographics and can market the target audience
accordingly.

Recommendation system is an example of semi-supervised learning. It uses
both structured and unstructured data like text, documents, audio and
video to recommend movies or songs that the customer would like.

Figure 6 and Figure 7 shows a visual representation of how the different
ML algorithms work.

![The Supervised and Unsupervised ML algorithms](/assets/images/intro_strategy/media/image6.png)

Figure 6: The Supervised and Unsupervised ML algorithms

![Decision Tree and Adaboost](/assets/images/intro_strategy/media/image7.png)

Figure 7: Decision Tree and AdaBoost

![Random Forest containing 6 decision trees](/assets/images/intro_strategy/media/image8.png)

Figure 8: Random Forest containing 6 decision trees
### Deep Learning {#dl}

DL makes use of ANN (Artificial Neural Network) algorithm to produce the
output. ANN, in its simplest form, contains an input layer, a hidden
layer, and an output layer. Data is loaded into the input layer, weights
and biases associated with the independent variable goes through the
hidden layer where their calculations are performed, and finally
produces the output. The hidden layer can contain thousands of layers.
This layer processes weights and biases of several variables and the
output is optimized using stochastic(probability) processes. The advent
of ANN opened up new vistas in the world of AI and spun-out new kinds of
DL techniques. CNN (Convolution Neural Network), RNN (Recurring Neural
Network), LSTM (Long short term Memory), NLP (Natural Language
Processing), GNN (Graph Neural Networks), Autoencoders and Reinforcement
Learning (RL). These algorithms together form the non-generative aspect
of DL. Table 1 shows the different scenarios where they are being used.

<table>
<thead>
<tr class="header">
<th>DL Algorithms</th>
<th>Medical Image Analysis</th>
<th></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>CNN(Image processing, pattern recognition)</td>
<td>Detecting lung cancer in CT scans using ResNet</td>
<td></td>
</tr>
<tr class="even">
<td></td>
<td>Brain tissue segmentation</td>
<td></td>
</tr>
<tr class="odd">
<td></td>
<td><p>Coronary</p>
<p>Identify drug molecules for drug discovery</p>
<p>Object detection like pedestrians, traffic lights</p>
<p>Fraud detection</p></td>
<td></td>
</tr>
<tr class="even">
<td>LSTM(sequential data, long term dependencies)</td>
<td>Analyzing and learning sequential data</td>
<td></td>
</tr>
<tr class="odd">
<td></td>
<td><p>Analyzing anaomaly detection in ECG by identifying irregular heart rhythms</p>
<p>Forecasting equipment failure in power plants</p>
<p>Predict loan defaults based on customer transaction history</p></td>
<td></td>
</tr>
<tr class="even">
<td>RL(decision making, optimization)</td>
<td>Robotics</td>
<td></td>
</tr>
<tr class="odd">
<td></td>
<td>Autonomous Driving tasks like path planning and decision making in traffic</td>
<td></td>
</tr>
<tr class="even">
<td></td>
<td><p>Find molecular structures for new drug design</p>
<p>Optimizes treatment plans for patients</p>
<p>Trading bots for helping to maximize returns while minimizing risks</p></td>
<td></td>
</tr>
<tr class="odd">
<td>RNN(time-series, sequential data)</td>
<td><p>Predicting seizures using EEG time series</p>
<p>Analyzing drug response patterns using timeseries data</p>
<p>Predict stock prices based on historical data</p>
<p>Predicting electricity demand</p></td>
<td></td>
</tr>
<tr class="even">
<td>Autoencoders(anomaly detection, data compression)</td>
<td><p>Detecting rare disorders from MRI scan</p>
<p>Identifying sensor failures in self-driving car</p>
<p>Identifying unusual spending patterns in credit card transactions</p>
<p>Compressing high dimensional proteomics/gene expression data for analysis</p></td>
<td></td>
</tr>
<tr class="odd">
<td>GNN(ideal for networks relationships)</td>
<td>Identify disease related genes from gene interaction networks</td>
<td></td>
</tr>
<tr class="even">
<td></td>
<td>Predict how different drugs will interact with each other</td>
<td></td>
</tr>
<tr class="odd">
<td></td>
<td>Modeling city wide traffic networks for real-time optimization</td>
<td></td>
</tr>
</tbody>
</table>

Table 1. The different DL algorithms and their applicatons

CNN is used in spatial and visual representation. It is used in the
healthcare industry for diagnostic purposes as in detection of cancerous
tissues in radiology. It can also be used in object detection. RL is
used to make decisions based on interacting with the environment. For
example, it can be used in robots to help navigate objects. Autoencoders
is used in feature extraction while RNN is used for sequential data and
has been used in timeseries modelling and for text analysis. LSTM is an
example of RNN. These non-generative DL models are used for prediction
or analysis of data.

I used the CIFAR-10 database<sup>(5)</sup> and CNN to predict the
classification of the dataset. There were 10 classes in the dataset( ie
truck, dog, car, horse, yacht). Some of the pictures available in the
original dataset are shown below:

![Pictures of CIFAR dataset that are being used as an Input Layer in CNN](/assets/images/intro_strategy/media/image9.png)

Figure 9: Pictures of the CIFAR dataset that are being used as an Input
Layer in CNN

The visual representation of the architecture of CNN used is shown in
Figure 10  
  
![Architecture of CNN](/assets/images/intro_strategy/media/image10.png)

Figure 10. Architecture of CNN

*Note: Visualkeras package is used in representing the architecture*

The main components of CNN in this architecture, as shown in Figure 7,
are:

**Input layer**: This is the layer that contains input data. In this
case, it contains images

**Convolutions**: Essentially this is the layer that extracts the
various features from the input images.

**Maxpooling2D**: The main purpose of this layer is to decrease feature
map from the convolutions layer in order to reduce computational costs.
This is performed by decreasing the connections between layers and
independently operating on each feature map. There are 2 different kinds
of pooling: MaxPooling and AveragePooling

**Dropout**: The dropout layer ensures that if neurons are extracting
the same features then they give more significance to those features and
can contribute to overfitting the training data and hence the model. In
order to prevent this a dropout rate is used so that it drops some of
the neurons to zeros at each layer.

**Flatten**: It converts all the 2-Dimensional arrays from the
previously pooled feature maps into a one dimensional continuous linear
vector. The resulting flattened matrix is then fed as input to the fully
connected layer to classify the image.

**Fully Connected (FC**): This layer consists of the weights and biases
along with the neurons and is used to connect the neurons between two
different layers. These layers are usually placed before the output
layer to perform classification or regression. They usually form the
last few layers of a CNN Architecture.

**Dense**: This layer connects to the preceding layer which means the
neurons of the layer are connected to every neuron of its preceding
layer. The outputs of this layer are computed using a softmax activation
function, which determines the probabilities of the classes. If it is a
binary classification problem, a sigmoid activation function can be
used.

*Note: For a detailed understanding of CNN you can refer to this site:  
[Convolutional Neural Network: A Complete
Guide](https://learnopencv.com/understanding-convolutional-neural-networks-cnn/)*

The accuracy of the model in predicting has been found to be around 92%

![Accuracy of the CNN model for the CIFAR dataset](/assets/images/intro_strategy/media/image11.png)

Figure 11. Accuracy of the CNN model for the CIFAR dataset

Table 2 shows the prediction made by CNN of the actual class in the
Input images.

| True Class | Predicted Class |
|------------|-----------------|
| cat        | cat             |
| ship       | ship            |
| ship       | automobile      |

Table 2: Prediction of the pictures made by CNN

The arrival of generative AI, more specifically LLM (Large Language
Models) has generated a buzz across most industries and garnered lots of
interest with ease of use like the ChatGPT (GPT: Generative Pre-trained
Transformer) of OpenAI. Compared with the non-generative models,
Generative AI can generate texts, images, audio and even code based on a
language prompt. One does not even have to know codes to generate the
output that was once unthinkable. GAN (Generative Adversarial Network),
Variational Autoencoders(VAE), Transformers are some of the algorithms
of DL.

Like the game theory, GAN uses a generative and a discriminative network
to mimic real data. The generator tries to produce fake data to fool the
discriminator. The latter’s part is to recognise that and failing to do
so it improves on its learning. The constant adversarial nature of the
duo helps improve the output till the time the discriminator can
distinguish the fake from the real one.

The main components of GAN are:  
**Generator (G):** It uses random noise(also known as latent space) to
transform into realistic data to fool the Discriminator.

**Discriminator (D):** It takes the data from the Generator and try to
distinguish the fake from the real using a probability score of 1 or 0.
Both the Generator and the Discriminator uses neural network

**Loss Function:** There are 2 loss functions; one for the generator
where it tries to produce more realistic images by maximizing the
Discriminator’s error. The Discriminator on the other hand tries to
minimize the error by correctly distinguishing real and fake images.

**Training Process:** This adversarial process continues till the
Generator can produce realistic images. This is usually done where the
loss that is computed and feedback to the network by a process called
backpropagation

GANs can be used in image synthesis, data augmentation,
super-resolution, and more. By combining variant architecture like DCGAN
it can be used to generate drug molecules thereby reducing the timeline
of drug production. Other applications like generating synthetic data,
image analysis and producing videos from text are some of the examples
of GAN.

VAE encodes input data into a latent space and decodes it back, making
them effective in generating new instances similar to the original data.

I used the same data(CIFAS 10 Image data set) as I used CNN to generate
images of higher quality. The actual data set is the one that the
discriminator knows. Figure 12 displays the initial noise that the
generator uses to produce the fake data.

![The latent space(noise) used by the generator](/assets/images/intro_strategy/media/image12.png)

Figure 12: The latent space(noise) used by the generator

The final images generated are blurry compared to the original dataset
(please see Figure 9) after 100 epochs. This means by increasing the
number of epochs. we can increase the quality of the data to make it
more realistic.

*Note: An epoch is a term that is used by the number of times the data
cycles through the neural network.*

![The final images generated](/assets/images/intro_strategy/media/image13.png)

Figure 13. The final images generated.

LLM (Large Language Models) on the other hand uses the transformer
architecture. Transformers use both the encoder and the decoder and has
self-attention mechanism that helps to understand the
context<sup>(6)</sup> in which the text has been organized. The
architecture of the transformer is shown in Figure 14<sup>(6)</sup>.

![Architecture of the transformer](/assets/images/intro_strategy/media/image14.png)

Figure 14. Architecture of the transformer as illustrated in the
article: “Attention is all you need”<sup>(6)</sup>

*Note: For an excellent visual explanation of how the tranformer technology works please refer to "Hands-on Large Language Models" by Jay Alammar & Maarten Grootendorst.<sup>(8)</sup>. Additionally, the latest architecture diagrams of the different LLM models are demonstrated in an excellent article, "The Big LLM Architecture Comparison".<sup>(9)</sup>*


The components of the transformer are:

**Encoder:** Processes input sequences and extracts meaningful
representations.

**Decoder:** Generates output sequences using the encoder’s
representations.

**Self-Attention Mechanism:** This is the crucial part. The purpose of
this mechanism is to enable the model to weigh the importance of words
in a sentence relative to each other. Under the hood it uses Queries,
Keys and Values that are projections of the input embeddings. Using the
softmax function it then assigns scores and emphasizes those tokens that
are important.

*Note: Both the decoder and the encoder contains 6 layers. The encoder
contains 2 sub-layers while the decoder has an additional sub-layer
which contains an additional Multi-Head Attention**.** In this case, the
multiple attention layers run in parallel while capturing diverse
patterns and relationships and thereby making the model robust as each
head learns different dependencies.*

### Feedforward Networks {#feedforward}
The purpose of this step is to make the transformer understand complex
relationship between words. It enhances information processing before
passing it to the next layer. The output from the previous attention
layer is passes through the layers and applies an activation function.

*Note: Neural network contains activation functions which actually makes
the neurons work by introducing non-linearity in the network allowing it
to learn and find patterns. There are several activations functions like
Relu, LeakyRelu, Softmax, tanh. In this case, the activation function
that is applied is Relu while in the self-attention mechanism it is
softmax.*

### Positional Encoding {#positional-encoding}

It helps the transformer to learn the sequence of each word in a
sentence.

### Layer Normalization {#layer-norm}

It helps the transformer to train faster and perform better by ensuring
that each sub-layer ‘s training is stabilised by adjusting values inside
each layer so they stay within a stable range.

LLMs are built on foundational models. These models are built on
pre-trained data that are then used to perform tasks as shown in Figure
15. There are 2 ways to improve the accuracy of the foundational model:
RAG(Retrieval Augmented Generation) and fine-tuning.

![Pre-trained foundational model](/assets/images/intro_strategy/media/image15.png)

Figure 15. Pre-trained foundational model containing massive datasets
used to perform downstream tasks<sup>(7)</sup>.

LLMs like the GPTs are useful in text summarization, creation of
chatbots and virtual assistants, knowledge discovery while DALL-E, a
type of Language Model, combines transformers with visual embeddings to
generate images from text descriptions.

LLMs have limitations like the training model used has a cut-off date
and hence will not have the latest information, providing inaccurate
responses. Most organizations have some kind of domain knowledge
captured in the form of documents, tacit knowledge even in the form of a
knowledge base, ontologies as in biomedical ontology. By integrating
these sources with the LLM it is possible to optimize, and thereby
provide an accurate, response that is geared towards the specific task.
RAG interact with the environment and as agents (agents are lightweight
applications) help to automate the power by creating tasks like
answering emails, conversational chatbots and even converting text
messages to SQL (Structured Query Language) based queries without

*Note: SQL is used in relational databases*

the need to retrain the model! Hence it can also be cost-effective.
Added to this is the advent of multi-modal agents which combines audio,
video and text to generate an interactive experience for the customer.
There are open-source models that can be used to drive value for the
customers. Llama is an open source model from Meta. Hugging face has a
plethora of open source models that can be also used. In the next
chapter, I will use an open source model to find out the current
situation of the operation of the business to identify how a quick win
can be generated to win trust.

## Strategic Purpose and Value Proposition {#strategic-purpose}

The word strategy came from the military. Like the military, all aspects
of the organization and its concomitant ecosystem like infrastructure,
marketing, finance, sales, governance has to gel and link in order to
serve the purpose for what the business has been set up in the first
place. A strategy in its elemental form will contain vision, values,
purpose, value proposition that connects the triad of people, process
and technology to serve the needs of the customer that will be
financially feasible and sustainable.

The vision statement delineates what the organization would like to be
in the future. The purpose of an organization defines the overarching
mission of the organization’s existence. It is vital when crafting a
strategy document for an organization that the purpose is thought
through and stated clearly as it defines the existence of the business.
It calls into question the aspirational nature of the business; who it
wants to target. When preparing the purpose statement, choices need to
be made so that the target that it wants to accomplish, be it societal,
demographics or financial is clearly articulated. Choices in the case
constitutes selecting products or services that caters to a specific
audience. Hence, purpose tells *why* the company is doing what it is
doing. As such, purpose is the reason why the company came into being.
Products and/or services that the company provides therefore become the
manifestation of how purpose is defined. Product design and delivery of
services are an extension of the purpose. For example, Apple’s purpose
is ‘*“To bringing the best user experience to customers through
innovative hardware, software, and services”*.

*Note: In some companies you will find a Mission statement instead. The
Purpose of a company does not change over time. It is stable and remains
constant. The Mission statement on the other hand can change over time.
It defines the Purpose with specific goals on how to achieve them.*

The value proposition constitutes the 4Ps (products, prices, places and
promotion) tells us what are the differentiators that makes it
attractive to the customer. Value proposition acts as the bridge to the
purpose through these tangible attributes that provides outcomes to
customers. For example, the value proposition of Walmart is *Everyday
low prices (EDLP)* and this governs how everything from supplier to
customer service works in synch to provide value to the customer.

Some of the examples of Purpose and VP of companies dealing with AI are
listed below:

**1. OpenAI**

**Purpose**:  
“To ensure that artificial general intelligence (AGI) benefits all of
humanity.”  
OpenAI aims to create safe and broadly beneficial AI, aligning with its
long-term goal of ethical and equitable advancements in AI technology.

**Value Proposition**:  
OpenAI provides cutting-edge AI tools and models (like GPT, Codex, and
DALL·E) that empower developers, businesses, and individuals to solve
complex problems, automate tasks, and innovate in creative, technical,
and operational domains.

**2. Microsoft**

**Purpose**:  
“To empower every person and every organization on the planet to achieve
more.”  
Microsoft seeks to enable innovation and productivity through
technology, fostering accessibility and inclusivity in the digital
world.

**Value Proposition**:  
Microsoft offers an ecosystem of software (e.g., Windows, Office), cloud
services (Azure), and AI solutions that integrate seamlessly, enhance
productivity, and drive digital transformation for businesses and
individuals worldwide.

**3. Hugging Face**

**Purpose**:  
“To democratize AI by building tools and models that make AI accessible
to everyone.”  
Hugging Face is committed to open-source collaboration and the ethical
advancement of AI technologies for researchers, developers, and
enterprises.

**Value Proposition**:  
Hugging Face provides an open-source platform, pre-trained machine
learning models, and tools (like the Transformers library) that simplify
the use and deployment of state-of-the-art AI models in research and
real-world applications.

**4. Anthropic**

**Purpose**:  
“To build reliable, interpretable, and steerable AI systems that advance
human safety and values.”  
Anthropic focuses on developing AI systems designed with safety and
alignment at the forefront, aiming to minimize risks and maximize
ethical applications.

Anthropic offers expertise and frameworks in designing AI systems with
an emphasis on alignment, transparency, and reliability, ensuring safer
AI integration into businesses and societal infrastructure.

While economics and the profit consideration are an important aspect in
any company, it’s the purpose that drives what the company wants to do
and how it will generate value. It is the authentic sense of what the
company is, and that includes all the departments, to serve the purpose.
Strategy is all about leadership. The hallmark of a true leader is what
is known as the 3Cs(Character, commitment and competencies). Ultimately,
a true leader will delve deep into the purpose, the being of the
company, to ensure that products and services are aligned with the
purpose. The purpose statement made in the examples cited above speaks
about the reason the company has been formed in the first place.

In the next chapter, I will discuss how to explore the current status of
the organization, find the gaps in the current process and what actions
can be taken to generate value.

## References {#references} 
1.
<https://pages.stern.nyu.edu/~adamodar/New_Home_Page/dataarchived.html#returns>

2\. “The Five Competitive Forces That Shape Strategy”, Michael E.
Porter, Harvard Business Review, January 1, 2008

3\. “How Much Does Industry Matter, Really?”, Strategic Management
Journal, Summer 1977, pp.15-30

4\. [GAN Lab: Play with Generative Adversarial Networks in Your
Browser!](https://poloclub.github.io/ganlab/)

5\. https://www.cs.toronto.edu/\~kriz/cifar.html

6\. “Attention is all you need”, [Ashish
Vaswani](https://arxiv.org/search/cs?searchtype=author&query=Vaswani,+A), [Noam
Shazeer](https://arxiv.org/search/cs?searchtype=author&query=Shazeer,+N), [Niki
Parmar](https://arxiv.org/search/cs?searchtype=author&query=Parmar,+N), [Jakob
Uszkoreit](https://arxiv.org/search/cs?searchtype=author&query=Uszkoreit,+J), [Llion
Jones](https://arxiv.org/search/cs?searchtype=author&query=Jones,+L), [Aidan
N.
Gomez](https://arxiv.org/search/cs?searchtype=author&query=Gomez,+A+N), [Lukasz
Kaiser](https://arxiv.org/search/cs?searchtype=author&query=Kaiser,+L), [Illia
Polosukhin](https://arxiv.org/search/cs?searchtype=author&query=Polosukhin,+I)
<https://arxiv.org/abs/1706.03762>

7\. On the Opportunities and Risks of Foundational Models,
[\[2108.07258\] On the Opportunities and Risks of Foundation
Models](https://arxiv.org/abs/2108.07258)

8\. Hands-on large language models: language understanding and generation, Alammar, Jay., Grootendorst, Maarten, Sebastopol, CA., O'Reilly Media, Inc.

9\. [Sebastian Raschika]The Big LLM Architecture Comparison.(https://magazine.sebastianraschka.com/p/the-big-llm-architecture-comparison)